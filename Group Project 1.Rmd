---
output:
  pdf_document: default
  html_document: default
---
--
title: 'Group Project #1'
author: "Bianca Brusco, Clare Clingain, Kaushik Mohan, & Frankie Wunschel"
date: "April 10, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
require(lme4)
require(lattice)
require(lmerTest)
library(ggplot2); vanillaR <- F
classroom <- read.csv("classroom.csv")
```

#Part 1: Frankie


##Create 1st grade variable

```{r}
classroom <- classroom %>% mutate(Math1 = mathkind + mathgain)
```

##Random Intercepts for classroom, nested in schools UMM

We begin our analysis by looking at the UMM with random intercepts for schools and classrooms, i.e. :

$$Math1st_{ijk} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk}$$

$\zeta_{k} \sim N(0, \sigma_\zeta^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all are independent of each other


```{r}
model1 <- lmer(Math1~(1|schoolid/classid),data=classroom)
summary(model1)
```

$$ICC_{class}=\frac{85.46}{1146.8+280.68+85.46}\approx.056$$
$$ICC_{school}=\frac{280.68}{1146.8+280.68+85.46}\approx.186$$

We hence find, from the fit summary above, that the equation for our model is:


$$Math1st_{ijk} = 522.54 + \zeta_{k} + \eta_{jk} + \epsilon_{ijk}$$

$\zeta_{k} \sim N(0, 280.68), \eta_{jk} \sim N(0, 85.46),$ and $\epsilon_{ijk} \sim N(0, 1146.80)$, all are independent of each other



##Model with School Level Predictors Added

We then add all the school level predictors (that is, "housepov") and report below the model fit :

```{r}
model2 <- lmer(Math1~housepov+(1|schoolid/classid),data=classroom)
summary(model2)
anova(model1, model2, refit = F)
```

Report the changes in the variances of the random effects:

Change in $\sigma_\zeta^2$: decreased to 250.93 from 280.63
$\sigma_\eta^2$ decreases to 82.36 from 85.46 
$\sigma_\epsilon^2$ slightly increases to 1146.95 from 1146.8

The ANOVA/LRT has a p-value of almost zero, $p = 3.39e-05$ , thus we reject the $H_0$ at our $\alpha = 0.05$. That is, we find evidence that it makes sense to include the school level predictor, housepov.

##Model with all Class Level Predictors Added

We now re-run the model after including all the classroom level predictors, that is "mathknow", "yearstea", "mathprep", and report the model fit. 

```{r}
model3 <- lmer(Math1~housepov+mathknow+yearstea+mathprep+
                 (1|schoolid/classid),data=classroom)
summary(model3)
```

## creating reducted dataset taking away missing data

The variable of interest "mathknown" includes some missing values. The model for which we have reported the summary above therefore removes the observations for which missing data is present. 

To be able to compare Model 2 (with school level predictors) with Model 3 (with both school level and classroom level predictors), we removed from the dataset students that had missing values, creating a reduced dataset. This left us with a sample of 1081 students. We then re-run model 2 on this reducted dataset and compared it to Model 3. 

```{r}
classroom_red = na.omit(classroom)
model2_red <- lmer(Math1~housepov+(1|schoolid/classid),data=classroom_red)
model3_red <- lmer(Math1~housepov+mathknow+yearstea+mathprep+
                     (1|schoolid/classid),data=classroom_red)

summary(model3_red)
anova(model2_red, model3_red, refit = F)
```

Change in $\sigma_\epsilon^2$ and $\sigma_\eta^2$: 
$\sigma_\epsilon^2$ decreased to 1136.43, 
$\sigma_\eta^2$ increased to 94.36; 
$\sigma_\zeta^2$ = 223.31

A possible reason why $\epsilon$ decreased in this model, but not $\eta$ is that adding the classroom level predictors makes it so that more of the overall variation is explained by "structured" variation (that is, related to the fact that students are in different classrooms) rather than by unstructured ($\epsilon$), so that the latter decreases. However, we also have to note that in this case we are using the reduced dataset, so that some of the changes may be due to the fact that we are using two slightly different datasets. 

The anova test comparing the school level predictor to the model with the classroom predictors has a p-value 0.087, so we fail to reject the null hypothesis at our $\alpha = 0.05$ and conclude that adding classroom level predictors is not necessary, as it does not signficantly improve the model.


## Add all student-level predictors

We now include all the student level predictors in our model:

```{r}
model4 <- lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                 ses+(1|schoolid/classid),data=classroom)
summary(model4)
```

We test this new block compared to the model with both school-level and classroom level predictors. 

```{r}
anova(model3, model4, refit = F)
```

The LRT test between this two models has a p-value < $2.2*10^{-16}$. Therefore, at our $\alpha = 0.05$, we reject the null hypothesis and conclude that adding this black of predictors is justified. 


Changes in variance components :

$\sigma_\epsilon^2$ decreased to 1064.95,
$\sigma_\eta^2$ decreased to 93.89, and
$\sigma_\zeta^2$ decreased to 169.45.

We note that adding student-level predictors leads to a decrease in the overall variance of the model. By "controlling" for student-related variables, we also explain the between schools, as students with similar attributes might be similar across schools, hence reducing the overall variance of $\zeta$. 

The final model, with all school level, classroom level, and student level predictors, is:



$$Math1st_{ijk} = 539.63 + \zeta_{k} + \eta_{jk} + \epsilon_{ijk}  -17.65 * Housepov_k +1.35*Mathknow_{jk} +$$
$$0.01*YearsTea_{jk} - 0.27* Mathprep_{jk} -0.19* sex_{ijk} + -0.32* minority_{ijk} -0.12*ses_{ijk}$$

With:

$\zeta_{k} \sim N(0, \sigma_\zeta^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all are independent of each other


From the model fit above therefore we find that the fitted model is:


$$Math1st_{ijk} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk} + \beta_1Housepov_k +\beta_2Mathknow_{jk} +$$
$$\beta_3YearsTea_{jk} +\beta_4Mathprep_{jk} + \beta_5sex_{ijk} + \beta_6minority_{ijk} + \beta_7ses_{ijk}$$

With:

$\zeta_{k} \sim N(0, 169.45), \eta_{jk} \sim N(0, 93.89),$ and $\epsilon_{ijk} \sim N(0, 1064.95)$, all are independent of each other.


##Random Slope for Teacher-level predictor varying at school-level

We try adding a random slope for each teacher level predictor (varying at the school level; one by one - not all together). 

**mathknown**  
  
  
```{r}
rst.1 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+mathknow||schoolid)+(1|classid),data=classroom)
summary(rst.1)
ranova(rst.1,refit=F)
```

There is no need for the random slope for math knowledge at a school level as the p value = 0.99 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

**yearstea**  
  

```{r}
rst.2 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+yearstea||schoolid)+(1|classid),data=classroom)
summary(rst.2)
ranova(rst.2, refit=F)
```

There is no need for the random slope for "yeartea" at a school level as the p value = 0.93 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

```{r}
rst.3 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+mathprep||schoolid)+(1|classid),data=classroom)
summary(rst.3)
ranova(rst.3, refit=F)
```

There is no need for the random slope for "housepov" at a school level as the p value = 1 for the Chi-square test is not  signifcant at $\alpha = 0.05$.


\textbf{Question:} Why housepov bad idea?

\textbf{Answer:} There is only one data point per school, so we do not have enough information to calculate the slope for each school.

##Allowing correlations with random intercepts

## ONE BY ONE

Again, we add random slopes for each teacher-level predictor varying at the school level, but this time by allowing them to be correlated with the random intercepts. 

**mathknown**  


```{r}
rstc.1 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+mathknow|schoolid)+(1|classid),data=classroom)
summary(rstc.1)
ranova(rstc.1, refit=F)
```

There is no need for the random slope for math knowledge at a school level as the p value = 0.99 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

**yearstea**  

```{r}
rstc.2 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+yearstea|schoolid)+(1|classid),data=classroom)
summary(rstc.2)
ranova(rstc.2,refit=F) 
```

There is no need for the random slope for yearstea at a school level as the p value = 0.054 for the Chi-square test is not  signifcant at $\alpha = 0.05$.


**mathprep**

```{r}
rstc.3 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+mathprep|schoolid)+(1|classid),data=classroom)
summary(rstc.3)
ranova(rstc.3, refit=F)
```

There is no need for the random slope for mathprep at a school level as the p value = 0.09 for the Chi-square test is not  signifcant at $\alpha = 0.05$.


\textbf{Question:} Anything unusual about the variances? Why might this have occurred? (hint: what did you add to the model?)

\textbf{Answer:} We note that the model did not estimate the correlation parameter correctly for the models with random slopes for mathknown and mathprepr. Indeed, with a correaltion of respectively 1 and -1 with the random intercept, the parameter is a linear function of the variance component for the slope. This could be due to the fact that there is not enough classrooms in the schools (as we are adding random effects at the school levels, for classroom level predictors), so that there is not enough degrees of freedom, nor enough variation among the variables of interest, to calculate all the parameters required in the model. Obtaining a correlation of 1 and -1 should warn us of the fact that the models generated should not be trusted.  
Why is the correlation between random intercept and slope then calculated for yearstea? This could be due to the fact that this variable has a larger range, so that it can be more robustly estimated for some of the schools and the correlation between random slope and intercept then estimated more accurately even for schools with few classes.


##Random slopes for student-level predictors varying at classroom level

We now repeat the exercise by adding student level predictors, varying at the classroom level. 

##ONE BY ONE


**sex**  


```{r}
rss.1 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+sex||classid)+(1|schoolid),data=classroom)
summary(rss.1)
ranova(rss.1, refit=F)
```

There is no need for the random slope for sex at the classroom level, as the p value = 1 for the Chi-square test is not  signifcant at $\alpha = 0.05$.


**minority**  

```{r}
rss.2 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+minority||classid)+(1|schoolid),data=classroom)
summary(rss.2)
ranova(rss.2, refit=F)
```


There is no need for the random slope for minority at the classroom level, as the p value = 1 for the Chi-square test is not  signifcant at $\alpha = 0.05$.


**SES**  


```{r}
rss.3 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+ses||classid)+(1|schoolid),data=classroom)
summary(rss.3)
ranova(rss.3, refit=F)
```

There is no need for the random slope for ses at the classroom level, as the p value = 0.206 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

\textbf{Question:} why is this a bad idea to include a classroom-level variable with random slopes at classroom-level?

\textbf{Answer:}  Because all of the observations for a class will be the same, so we will not be able to compute the classroom slopes for each classroom (as we will only have one point).


##Allowing for correlations with random intercepts

##ONE BY ONE


**sex**  

```{r}
rssc.1 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+sex|classid)+(1|schoolid),data=classroom)
summary(rssc.1)
ranova(rssc.1, refit=F)
```

There is no need for the (correlated) random slope for sex at the classroom level, as the p value = 0.779 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

**Minority**    


```{r}
rssc.2 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+minority|classid)+(1|schoolid),data=classroom)
summary(rssc.2)
ranova(rssc.2)
```

There is no need for the (correlated) random slope for minority at the classroom level, as the p value = 0.202 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

**SES**  
 
```{r}
rssc.3 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+ses|classid)+(1|schoolid),data=classroom)
summary(rssc.3)
ranova(rssc.3)
```

There is no need for the (correlated) random slope for minority at the classroom level, as the p value = 0.147 for the Chi-square test is not  signifcant at $\alpha = 0.05$.

# B: -- reviewed up to here. 

##Random slopes for student-level predictors varying at school level

##ONE BY ONE
###Sex

```{r}
rss.4 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+sex||schoolid)+(1|classid),data=classroom)
summary(rss.4)
ranova(rss.4, refit=F)
```

The uncorrelated sex random slope at a school level is insignifcant with a p value of .433.

###Minority

```{r}
rss.5 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+minority||schoolid)+(1|classid),data=classroom)
summary(rss.5)
ranova(rss.5,refit=F)
```

The uncorrelated minority random slope at school level is insignificant with a pvalue of 1.0.

###SES

```{r}
rss.6 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
               ses+(1+ses||schoolid)+(1|classid),data=classroom)
summary(rss.6) #IS SIG
ranova(rss.6,refit=F)
```

The uncorrelated ses random slope at school level is signifcant with a p value of .03.

##Allowing for correlations with random intercepts

##ONE BY ONE
###Sex

```{r}
rssc.4 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+sex|schoolid)+(1|classid),data=classroom)
summary(rssc.4)
ranova(rssc.4, refit=F)
```

The correlated sex random slope at school-level is insignificant with a pvalue of .394.

###Minority

```{r}
rssc.5 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+minority|schoolid)+(1|classid),data=classroom)
summary(rssc.5)
ranova(rssc.5,refit=F) #sig
```

The correlated minority random slope at school-level is significant with a pvalue of .0025.

###SES

```{r}
rssc.6 <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                ses+(1+ses|schoolid)+(1|classid),data=classroom)
summary(rssc.6)
ranova(rssc.6,refit=F) #not sig
```

The correlated ses random slope at school-level is Very close to significance but not quite there with a p-value of .0766.


\textbf{Question:} Report unusual changes in variance.

\textbf{Answer:} Perhaps most striking is the change in variance for the random slope term on minority. Previously, it was 0. However, it jumps to 343.13 in the correlated model. The variance for the random slope term on SES also increases, but the correlated random slope is not a significant addition to our model according to the rand test results.

##Complex model 

Take two predictors that had sig random slopes and add to model, test for need of one conditional on the other

-Minority is sig for correlated

-Ses is sig for uncorrelated

```{r}
complex <-lmer(Math1~housepov+mathknow+yearstea+mathprep+sex+minority+
                 ses+(0+ses|schoolid)+(1+minority|schoolid)+(1|classid),data=classroom)
summary(complex)
ranova(complex, refit=F)
```

\textbf{Question:} Is the more complex model (with both random slopes in it) justified?

\textbf{Answer:} The complex model is justified since the rand test shows that the random slopes are both statistically significant at the 0.05 level, the only question revolves around statistical significance justifying compared to the Bayesian approach that would push for a simpler model.

The equation for the complex model is given by the following:

$Math1st_{ijk} = \beta_0 + \beta_1*housepov_{k} + \beta_2*mathknow_{jk} + \beta_3*yearstea_{jk} + \beta_4*mathprep_{jk} + \beta_5*sex_{ijk} + \beta_{6k}*ses_{ijk} + \beta_{7k}*minority_{ijk} + \zeta_{0k} + \zeta_{6k} + \zeta_{7k} + eta_{jk} + \epsilon_{ijk}$

where $\zeta_{0k} \sim N(0, \sigma_{\zeta_0}^2),\zeta_{6k} \sim N(0, \sigma_{\zeta_6}^2), \zeta_{7k} \sim N(0, \sigma_{\zeta_7}^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all independent of each other.

------------------------------------------------------------------------------

```{r}
summary(model1)
```
$V_C$, $V_S$, and $V_E$
\textbf{Question:} For UCM, write down: $V_C$, $V_S$, $V_E$ for the three variance components (simply the estimates). Think of them as possibly varying with a covariate, though.

\textbf{Answer:} For the UCM, $V_C$ = 85.46, $V_S$ = 280.68, and $V_E$ = 1146.80

```{r}
summary(model4)
```

\textbf{Question:} For the most complicated (all fixed effects) random INTERCEPTS ONLY model, what are: $V_C$, $V_S$, $V_E$?

\textbf{Answer:} For the most complicated fixed effects model with only random intercepts, $V_C$ = 93.89, $V_S$ = 169.45, and $V_E$ = 1064.95.

\textbf{Question:} By what fraction did these each decrease with the new predictors in the model?

\textbf{Answer:} $V_C$ increased  $\frac{93.89}{85.46}$

                  $V_S$ decreased  $\frac{169.45}{280.68}$
                  
                  $V_E$ decreased $\frac{1064.95}{1146.80}$

```{r}
summary(rss.6)
```

\textbf{Question:} Now consider the model with a random slope in ses. What are: $V_C$, $V_S(ses=0)$, $V_E$ ? We need to list 'ses=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in ses at the school level, $V_C$ = 88.56, $V_S(ses=0)$ = 167.98, and $V_E$ = 1035.12.

\textbf{Question:} What are: $V_S(ses=-0.50)$, $V_S(ses=+0.5)$ ?

\textbf{Answer:} In this model, in which the random slope for SES is uncorrelated with the random school-level intercept, $V_S(ses=-0.50) = 167.98 + (-.5)^272.50 + 2(-.5)0167.9872.50 = 186.105$, and $V_S(ses=+0.5) = 167.98 + (.5)^272.50 + 2*(.5)0167.98*72.50 = 186.105$

```{r}
summary(rssc.5)
```

\textbf{Question:} Now consider the model with a random slope in minority. What are: $V_C$, $V_S(minority=0)$, $V_E$? We need to list 'minority=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in minority at the school level, $V_C$ = 86.69, $V_S(minority=0)$ = 381.20, and $V_E$ = 1039.39.

\textbf{Question:} What are: $V_S(minority=0.25)$, $V_S(minority=+0.50)$, $V_S(minority=+0.75)$?

\textbf{Answer:} In this model, in which the random slope for minority is correlated with the random school-level, intercept, $V_S(minority=0.25) = 381.20 + (0.25)^2343.13 + 2(0.25)(-0.83)\sqrt{381.20}*\sqrt{343.13} = 252.5549$,

$V_S(minority=+0.50) = 381.20 + (0.50)^2343.13 + 2(0.50)(-0.83)\sqrt{381.20}*\sqrt{343.13} = 166.801$, and

$V_S(minority=+0.75) = 381.20 + (0.25)^2343.13 + 2(0.25)(-0.83)\sqrt{381.20}*\sqrt{343.13} = 123.9384$.

```{r}
summary(complex)
```

\textbf{Question:} Now consider the model with a random slope in ses & minority. What are: $V_C$, $V_S(minority=0,ses=0)$, $V_E$? We need to list 'ses=0, minority=0' here, or we don't know how to use the slope variance.

\textbf{Answer:} For the model with a random slope in ses & minority, $V_C$ = 80.63, $V_S(minority=0,ses=0)$ = 404.54, and $V_E$ = 1009.73.

\textbf{Question:} What are: $V_S(ses=0,minority=0.50)$, $V_S(ses=0.50,minority=0)$, $V_S(ses= 0.50, minority= 0.50)$?

\textbf{Answer:} In this model, in which the random slope for ses is uncorrelated with the random intercept, but the random slope for minority is correlated with the random intercept,

$V_S(ses=0,minority=0.50) = 404.54 + (0)^274.93 + (0.50)^2336.04 + 200404.5474.93 + 2*(0.50)(-0.83)\sqrt{404.54}*\sqrt{336.04} = 182.5268$,

$V_S(ses=0.50,minority=0) = 404.54 + (0.50)^274.93 + (0)^2336.04 + 20.500404.5474.93 + 2*(0)(-0.83)\sqrt{404.54}*\sqrt{336.04} = 423.2725$

$V_S(ses= 0.50, minority= 0.50) = 404.54 + (0.50)^274.93 + (0.50)^2336.04 + 20.500404.5474.93 + 2*(0.50)(-0.83)\sqrt{404.54}*\sqrt{336.04} = 201.2593$

```{r}
summary(complex)
```

\textbf{Question:} In the last model, what is a "likely" (+/- 1 sd) range for $\eta_{0jk}$

\textbf{Answer:} For the complex model, the "likely" range for $\eta{0jk}$ is 71.651 to 89.609.

\textbf{Question:} Can we make a similar statement about $\zeta_{0k}$?

\textbf{Answer:} Mathmatically we can with a range of 384.427 to 424.653 though we can do this it doesn't make much sense due to the correlated nature of this with the minority variable the values wouldn't hold much meaning and are easily misinterpreted.

\textbf{Question:} If you had a large value for $\eta_{0jk}$, would you expect a large or small or "any" value for: the two random slope terms, $\zeta_{1k}$ and $\zeta_{2k}$ for ses and minority?

\textbf{Answer:} If you have a very large $\eta_{0jk}$ you would expect a small value for $\zeta_{1k}$ and $\zeta_{2k}$ but the $\zeta_{2k}$ would not be as small due to its negative correlation with our $\zeta_{0k}$ which is effected by our eta value.

\textbf{Question:} If you had a large value for $\zeta_{0k}$, would you expect a large or small or "any" value for: the two random slope terms, $\zeta_{1k}$ and $\zeta_{2k}$ for ses and minority (discuss each separately)?

\textbf{Answer:} For $\zeta_{1k}$ would increase in the same direction but it could be any value due to the lack of correlation, keeping in mind that $\zeta_{0k}$ will create a ceiling effect of sorts for $\zeta_{1k}$ . While $\zeta_{2k}$ would be very small because of the correlation because of the two variables are negatively correlated.