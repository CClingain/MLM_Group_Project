--
title: 'Group Project #1'
author: "Bianca Brusco, Clare Clingain, Kaushik Mohan, & Frankie Wunschel"
date: "April 10, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
require(lme4)
require(lattice)
require(lmerTest)
library(ggplot2); vanillaR <- F
classroom <- read.csv("data/classroom.csv")
```

#Part 1

##Create 1st grade variable
```{r}
classroom <- classroom %>% mutate(Math1st = mathkind + mathgain)
```

##UMM Model

```{r UMM}
#UMM model
model1 <- lmer(Math1st~(1|schoolid/classid),data=classroom)
summary(model1)
#the ICC
#class
85.46/(1146.8+280.68+85.46)
#school
280.68/(1146.8+280.68+85.46)
```

$Math1st_{ijk} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk}$
where $\zeta_{k} \sim N(0, \sigma_\zeta^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all independent of each other

##Add all school-level predictors

```{r school-level predictors}
model2 <- lmer(Math1st~housepov+(1|schoolid/classid),data=classroom)
summary(model2)
anova(model1, model2, refit = F) #is sig
```

Change in $\sigma_\zeta^2$: decreased to 250.93;  
$\sigma_\eta^2$= 82.36;  $\sigma_\epsilon^2$ = 1146.95

The anova test comparing model 1 to model 2 has a p-value : 3.39e-05, so we reject the null hypothesis at our $\alpha = 0.05$ and conclude that it makes sense to include the school level predictor, housepov. 

##Add all classroom-level predictors

```{r classroom-level predictors}
model3 <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+(1|schoolid/classid),data=classroom)
summary(model3)


## CHECK...?


## creating reducted dataset taking away missing data
classroom_red = na.omit(classroom)
model2_red <- lmer(Math1st~housepov+(1|schoolid/classid),data=classroom_red)
model3_red <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+(1|schoolid/classid),data=classroom_red)

anova(model2_red, model3_red, refit = F)
```

Change in $\sigma_\epsilon^2$ and $\sigma_\eta^2$: $\sigma_\epsilon^2$ decreased to 1136.43, $\sigma_\eta^2$ increased to 94.36; $\sigma_\zeta^2$ = 223.31

Why is epsilon reduced but not eta: explaining what's happening at student level but not classroom level. Moreover, adding class level predictors makes it so that more of the overall variation is explained by "structured" variation rather than by unstructured ($\epsilon$)
May increase because of sample decrease (missing data) --

The anova test comparing model 2 to model 3 has a p-value 0.08, so we fail to reject the null hypothesis at our $\alpha = 0.05$ and conclude that ??it does not make sense to include classsroom level predictors.???



##Add all student-level predictors

```{r student-level predictors}
#Add all student-level predictors
model4 <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid/classid),data=classroom)
summary(model4)

## to test it with model 2 since anova for mod 3 not sig

model4_red <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid/classid),data=classroom_red)

anova(model2_red, model4_red, refit = F)
anova(model3, model4, refit = F) #is sig
```

Change in zeta eta epsilon; $\sigma_\epsilon^2$ decreased to 1064.95, $\sigma_\eta^2$  decreased to 93.89, $\sigma_\zeta^2$ decreased to 169.45
School level may drop because students are similar in one school but different across schools

$\hat{math1st_{ijk}} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk} + \beta_1*Housepov_k +\beta_2*Mathknow_{jk} + \beta_3*YearsTea_{jk} + \beta_4*Mathprep_{jk} + \beta_5*sex_{ijk} + \beta_6*minority_{ijk} + \beta_7*ses_{ijk}$

The anova test comparing model 3 to model 4 has a p-value < 2.2e-16, so we reject the null hypothesis and conclude that it makes sense to include student level predictors. Moreover, the Chi-Sq test comparing model 2 to model 4 has a p-value < 2.2e-16, so we conclude that the model with student level predictors (as a block) improves compared to the model with only school-level predictors. 


##Random Slope for Teacher-level predictor varying at school-level

```{r random slope teachers}
#ONE BY ONE
#mathknow
rst.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathknow||schoolid)+(1|classid),data=classroom)
summary(rst.1)
rand(rst.1) #not sig
#yearstea
rst.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+yearstea||schoolid)+(1|classid),data=classroom)
summary(rst.2)
rand(rst.2)
#mathprep
rst.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathprep||schoolid)+(1|classid),data=classroom)
summary(rst.3)
rand(rst.3)
```

\textbf{Question:} Why housepov bad idea? 

\textbf{Answer:} There is only one data point per school, so we cannot have a random slope since we can't even calculate a slope.

##Allowing correlations with random intercepts

```{r correlation with random intercepts}
#ONE BY ONE
#mathknow
rstc.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathknow|schoolid)+(1|classid),data=classroom)
summary(rstc.1)
rand(rstc.1)
#yearstea
rstc.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+yearstea|schoolid)+(1|classid),data=classroom)
summary(rstc.2)
rand(rstc.2) #just over .05
#mathprep
rstc.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathprep|schoolid)+(1|classid),data=classroom)
summary(rstc.3)
rand(rstc.3)
```

\textbf{Question:} Anything unusual about the variances? Why might this have occurred? (hint: what did you add to the model?)

\textbf{Answer:} The random slope for mathknow greatly increases in the second model, which is probably due to its correlation with the random intercept at the school-level. 

##Random slopes for student-level predictors varying at classroom level

```{r random slope student-level}
#ONE BY ONE
#sex
rss.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex||classid)+(1|schoolid),data=classroom)
summary(rss.1)
rand(rss.1)
#minority
rss.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority||classid)+(1|schoolid),data=classroom)
summary(rss.2)
rand(rss.1)
#SES
rss.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses||classid)+(1|schoolid),data=classroom)
summary(rss.3)
rad(rss.3)
```

\textbf{Question:} why is this a bad idea to include a classroom-level variable with random slopes at classroom-level? 

\textbf{Answer:} It may not explain much variance. 

##Allowing for correlations with random intercepts

```{r correlations}
#ONE BY ONE
#sex
rssc.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex|classid)+(1|schoolid),data=classroom)
summary(rssc.1)
rand(rssc.1)
#minority
rssc.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority|classid)+(1|schoolid),data=classroom)
summary(rssc.2)
rand(rssc.2)
#SES
rssc.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses|classid)+(1|schoolid),data=classroom)
summary(rssc.3)
rand(rssc.3)
```

##Random slopes for student-level predictors varying at school level

```{r random slopes student-level}
#ONE BY ONE
#sex
rss.4 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex||schoolid)+(1|classid),data=classroom)
summary(rss.4)
rand(rss.4)
#minority
rss.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority||schoolid)+(1|classid),data=classroom)
summary(rss.5)
rand(rss.5)
#SES
rss.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses||schoolid)+(1|classid),data=classroom)
summary(rss.6) #IS SIG
rand(rss.6)
```

##Allowing for correlations with random intercepts

```{r correlations student-school}
#ONE BY ONE
#sex
rssc.4 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex|schoolid)+(1|classid),data=classroom)
summary(rssc.4)
rand(rssc.4)
#minority
rssc.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority|schoolid)+(1|classid),data=classroom)
summary(rssc.5)
rand(rssc.5) #sig
#SES
rssc.6 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses|schoolid)+(1|classid),data=classroom)
summary(rssc.6)
rand(rssc.6) #not sig
```

\textbf{Question:} Report unusual changes in variance.

\textbf{Answer:} Perhaps most striking is the change in variance for the random slope term on minority. Previously, it was 0. However, it jumps to 343.13 in the correlated model. The variance for the random slope term on SES also increases, but the correlated random slope is not a significant addition to our model according to the rand test results.

##Complex model better?

```{r}
#Take two predictors that had sig random slopes and add to model, test for need of one conditional on the other
#minority is sig for correlated
#ses is sig for uncorrelated
complex <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(0+ses|schoolid)+(1+minority|schoolid)+(1|classid),data=classroom)
rand(complex)
```

\textbf{Question:} Is the more complex model (with both random slopes in it) justified?

\textbf{Answer:} The complex model is justified since the rand test shows that the random slopes are both statistically significant at the 0.05 level.

The equation for the complex model is given by the following:

$ \hat{Math1st_{ijk}} = \beta_0 + \beta_1*housepov_{k} + \beta_2*mathknow_{jk} + \beta_3*yearstea_{jk} + beta_4*mathprep_{jk} + \beta_5*sex_{ijk} + \beta_{6k}*ses_{ijk} + \beta_{7k}*minority_{ijk} + \zeta_{0k} + \zeta_{6k} + \zeta_{7k} + eta_{jk} + \epsilon_{ijk}$

where $\zeta_{0k} \sim N(0, \sigma_{\zeta_0}^2),\zeta_{6k} \sim N(0, \sigma_{\zeta_6}^2), \zeta_{7k} \sim N(0, \sigma_{\zeta_7}^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all independent of each other.

## $V_C$, $V_S$, and $V_E$

\textbf{Question:} For UCM, write down: $V_C$, $V_S$, $V_E$ for the three variance components (simply the estimates). Think of them as possibly varying with a covariate, though.

\textbf{Answer:} For the UCM, $V_C$ = 85.46, $V_S$ = 280.68, and $V_E$ = 1146.80

\textbf{Question:} For the most complicated (all fixed effects) random INTERCEPTS ONLY model, what are: $V_C$, $V_S$, $V_E$?

\textbf{Answer:} For the most complicated fixed effects model with only random intercepts, $V_C$ = 93.89, $V_S$ = 169.45, and $V_E$ = 1064.95.

\textbf{Question:} By what fraction did these each decrease with the new predictors in the model?

\textbf{Answer:} 

\textbf{Question:} Now consider the model with a random slope in ses. What are: $V_C$, $V_S(ses=0)$, $V_E$ ? We need to list 'ses=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in ses at the school level, $V_C$ = 88.56, $V_S(ses=0)$ = 167.98, and $V_E$ = 1035.12.

\textbf{Question:} What are: $V_S(ses=-0.50)$, $V_S(ses=+0.5)$ ?

\textbf{Answer:} In this model, in which the random slope for SES is uncorrelated with the random school-level intercept, $V_S(ses=-0.50) = 167.98 + (-.5)^2*72.50 + 2*(-.5)*0*167.98*72.50 = 186.105$, and $V_S(ses=+0.5) = 167.98 + (.5)^2*72.50 + 2*(.5)*0*167.98*72.50 = 186.105$ 

\textbf{Question:} Now consider the model with a random slope in minority. What are: $V_C$, $V_S(minority=0)$, $V_E$? We need to list 'minority=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in minority at the school level, $V_C$ = 86.69, $V_S(minority=0)$ = 381.20, and $V_E$ = 1039.39.

\textbf{Question:} What are: $V_S(minority=0.25)$, $V_S(minority=+0.50)$, $V_S(minority=+0.75)$?

\textbf{Answer:} In this model, in which the random slope for minority is correlated with the random school-level, intercept, 
$V_S(minority=0.25) = 381.20 + (0.25)^2*343.13 + 2*(0.25)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 252.5549$, 

$V_S(minority=+0.50) = 381.20 + (0.50)^2*343.13 + 2*(0.50)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 166.801$, and

$V_S(minority=+0.75) = 381.20 + (0.25)^2*343.13 + 2*(0.25)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 123.9384$.

\textbf{Question:} Now consider the model with a random slope in ses & minority. What are: $V_C$, $V_S(minority=0,ses=0)$, $V_E$? We need to list 'ses=0, minority=0' here, or we don't know how to use the slope variance.

\textbf{Answer:} For the model with a random slope in ses & minority, $V_C$ = 80.63, $V_S(minority=0,ses=0)$ = 404.54, and $V_E$ = 1009.73.

\textbf{Question:} What are: $V_S(ses=0,minority=0.50)$, $V_S(ses=0.50,minority=0)$, $V_S(ses= 0.50, minority= 0.50)$?

\textbf{Answer:} In this model, in which the random slope for ses is uncorrelated with the random intercept, but the random slope for minority is correlated with the random intercept,

$V_S(ses=0,minority=0.50) = 404.54 + (0)^2*74.93 + (0.50)^2*336.04 + 2*0*0*404.54*74.93 + 2*(0.50)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 182.5268$,

$V_S(ses=0.50,minority=0) = 404.54 + (0.50)^2*74.93 + (0)^2*336.04 + 2*0.50*0*404.54*74.93 + 2*(0)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 423.2725$

$V_S(ses= 0.50, minority= 0.50) = 404.54 + (0.50)^2*74.93 + (0.50)^2*336.04 + 2*0.50*0*404.54*74.93 + 2*(0.50)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 201.2593$

\textbf{Question:} In the last model, what is a "likely" (+/- 1 sd) range for \eta_{0jk}

\textbf{Answer:} For the complex model, the "likely" range for \eta{0jk} is 71.651 to 89.609.

\textbf{Question:} Can we make a similar statement about \zeta_{0k}?

\textbf{Answer:} 

\textbf{Question:} If you had a large value for \eta_{0jk}, would you expect a large or small or "any" value for: the two random slope terms, \zeta_{1k} and \zeta_{2k} for ses and minority?

\textbf{Answer:} 

\textbf{Question:} If you had a large value for \zeta_{0k}, would you expect a large or small or "any" value for: the two random slope terms, \zeta_{1k} and \zeta_{2k} for ses and minority (discuss each separately)?

\textbf{Answer:} 

# Part 2

##Running initial model

```{r pt2 q1}
#remove missing data -- not ideal, but have to do it for this analysis
classroom2 <- na.omit(classroom)
#model
new1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid)+(1|classid),data=classroom2)
```

##Residual that removes only the "fixed effects"
*manually construct the residual that removes only the 'fixed effects'
*hint: predict yhat, xb will generate the prediction for the outcome based on the fixed effects only
* then subtract it from the outcome; call this residual: resFE
```{r pt2 q2}
#predicted scores
pred.yhat <- predict(new1,re.form=~0)

#residual
resFE <- classroom2$Math1st-pred.yhat

#show that it's not independent
if (vanillaR) {
ord <- order(unlist(tapply(classroom2$mathkind, classroom2$schoolid, median)))
boxplot(split(classroom2$mathkind, classroom2$schoolid)[ord])
} else {
ggplot(classroom2, aes(x = reorder(schoolid, Math1st, FUN = median), y = Math1st)) +
geom_boxplot()
}
```

##Residuals for BLUPs random effects

```{r pt2 q3}
#getting predicted zeta_0 and eta_0
ranefs <- ranef(new1)
zeta0 <- ranefs$schoolid[,1]
eta0 <- ranefs$classid[,1]
#indexing
idx.sch <- match(classroom2$schoolid, sort(unique(classroom2$schoolid))) 
idx.cls <- match(classroom2$classid, sort(unique(classroom2$classid))) 
classroom2$zeta0 <- zeta0[idx.sch]
classroom2$eta0 <- eta0[idx.cls]
#now subtract all from outcome
resFE_RE <- classroom2$Math1st-pred.yhat-classroom2$zeta0-classroom2$eta0
#show that it's not independent, but much less correlated than resFE
if (vanillaR) {
ord <- order(unlist(tapply(resFE_RE, classroom2$schoolid, median)))
boxplot(split(resFE_RE, classroom2$schoolid)[ord])
}else{
ggplot(classroom2, aes(x = reorder(schoolid, resFE_RE, FUN = median), y = resFE_RE)) +
geom_boxplot()
}
```

## Examining BLUPs for normality

```{r blups normality, fig.width=6, fig.height=4}
par(mfrow=c(1,2))
plot(density(zeta0), main ="Normality Check for Zeta")
plot(density(eta0), main = "Normality Check for Eta")
#look pretty normal
qqnorm(zeta0);qqline(zeta0)
qqnorm(eta0);qqline(eta0)
#looking good
```

##Simulation
*if In a simpler setting, with no classroom level, if H0: \sigma^2_{\zeta_0}=0 were true, and you sampled 100 schools of size 10, what would be a potential expected estimate of \sigma_{\zeta_0} if you know that \sigma_{\epsilon}=1 ?  HINT: the Central Limit Theorem applies.  Simulate to find out.
```{r blups simulation}
schools <- rep(10,100)
school.sim <- rnorm(100,0,1) #no idea if this is the appraoch we should take??
```

##New Complex Model

```{r new complex model}
classroom <- read.csv("data/classroom.csv")
classroom2 <- na.omit(classroom)
newcomplex <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(minority|schoolid)+(1|classid),data=classroom2)
```

##Manually calculate residuals for fixed effects

```{r complex resFE}
#predicted scores
pred.yhat2 <- predict(newcomplex,re.form=~0)

#residual
resFE2 <- classroom2$Math1st-pred.yhat2

#show that it's not independent
if (vanillaR) {
ord <- order(unlist(tapply(classroom2$Math1st, classroom2$schoolid, median)))
boxplot(split(classroom2$Math1st, classroom2$schoolid)[ord])
} else {
ggplot(classroom2, aes(x = reorder(schoolid, Math1st, FUN = median), y = Math1st)) +
geom_boxplot()
}
```

##Residuals from BLUPs random effects

```{r complex blup residuals}
#getting predicted zeta_0 and eta_0
ranefs2 <- ranef(newcomplex)
zeta0c <- ranefs$schoolid[,1]
eta0c <- ranefs$classid[,1]
#indexing
classroom2$zeta0c <- zeta0c[idx.sch]
classroom2$eta0c <- eta0c[idx.cls]
#now subtract all from outcome
resFE_RE2 <- classroom2$Math1st-pred.yhat-classroom2$zeta0c-classroom2$eta0c
#show that it's not independent, but much less correlated than resFE
if (vanillaR) {
ord <- order(unlist(tapply(resFE_RE2, classroom2$schoolid, median)))
boxplot(split(resFE_RE2, classroom2$schoolid)[ord])
}else{
ggplot(classroom2, aes(x = reorder(schoolid, resFE_RE2, FUN = median), y = resFE_RE2)) +
geom_boxplot()
}
```

##Examining Normality of BLUPs

```{r complex blup normality}
par(mfrow=c(1,2))
plot(density(zeta0c), main ="Normality Check for Zeta")
plot(density(eta0c), main = "Normality Check for Eta")
#look pretty normal
qqnorm(zeta0c);qqline(zeta0c)
qqnorm(eta0c);qqline(eta0c)
#looking good, but with a few possible outliers
```

##Tracking down outliers

```{r complex outliers}

```

