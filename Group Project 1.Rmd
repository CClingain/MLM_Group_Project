--
title: 'Group Project #1'
author: "Bianca Brusco, Clare Clingain, Kaushik Mohan, & Frankie Wunschel"
date: "April 10, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
require(lme4)
require(lattice)
require(lmerTest)
library(ggplot2); vanillaR <- F
classroom <- read.csv("classroom.csv")
```

#Part 1: Frankie

##Create 1st grade variable
```{r}
classroom <- classroom %>% mutate(Math1st = mathkind + mathgain)
```

##UMM Model

```{r UMM}
#UMM model
model1 <- lmer(Math1st~(1|schoolid/classid),data=classroom)
summary(model1)
#the ICC
#class
85.46/(1146.8+280.68+85.46)
#school
280.68/(1146.8+280.68+85.46)
```

$Math1st_{ijk} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk}$
where $\zeta_{k} \sim N(0, \sigma_\zeta^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all independent of each other

##Add all school-level predictors

```{r school-level predictors}
model2 <- lmer(Math1st~housepov+(1|schoolid/classid),data=classroom)
summary(model2)
anova(model1, model2, refit = F) #is sig
```

Change in $\sigma_\zeta^2$: decreased to 250.93;  
$\sigma_\eta^2$= 82.36;  $\sigma_\epsilon^2$ = 1146.95

The anova test comparing model 1 to model 2 has a p-value : 3.39e-05, so we reject the null hypothesis at our $\alpha = 0.05$ and conclude that it makes sense to include the school level predictor, housepov. 

##Add all classroom-level predictors

```{r classroom-level predictors}
model3 <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+(1|schoolid/classid),data=classroom)
summary(model3)


## CHECK...?


## creating reducted dataset taking away missing data
classroom_red = na.omit(classroom)
model2_red <- lmer(Math1st~housepov+(1|schoolid/classid),data=classroom_red)
model3_red <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+(1|schoolid/classid),data=classroom_red)

anova(model2_red, model3_red, refit = F)
```

Change in $\sigma_\epsilon^2$ and $\sigma_\eta^2$: $\sigma_\epsilon^2$ decreased to 1136.43, $\sigma_\eta^2$ increased to 94.36; $\sigma_\zeta^2$ = 223.31

Why is epsilon reduced but not eta: explaining what's happening at student level but not classroom level. Moreover, adding class level predictors makes it so that more of the overall variation is explained by "structured" variation rather than by unstructured ($\epsilon$)
May increase because of sample decrease (missing data) --

The anova test comparing model 2 to model 3 has a p-value 0.08, so we fail to reject the null hypothesis at our $\alpha = 0.05$ and conclude that ??it does not make sense to include classsroom level predictors.???



##Add all student-level predictors

```{r student-level predictors}
#Add all student-level predictors
model4 <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid/classid),data=classroom)
summary(model4)

## to test it with model 2 since anova for mod 3 not sig

model4_red <- lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid/classid),data=classroom_red)

anova(model2_red, model4_red, refit = F)
anova(model3, model4, refit = F) #is sig
```

Change in zeta eta epsilon; $\sigma_\epsilon^2$ decreased to 1064.95, $\sigma_\eta^2$  decreased to 93.89, $\sigma_\zeta^2$ decreased to 169.45
School level may drop because students are similar in one school but different across schools

$\hat{math1st_{ijk}} = \beta_{0ijk} + \zeta_{k} + \eta_{jk} + \epsilon_{ijk} + \beta_1*Housepov_k +\beta_2*Mathknow_{jk} + \beta_3*YearsTea_{jk} + \beta_4*Mathprep_{jk} + \beta_5*sex_{ijk} + \beta_6*minority_{ijk} + \beta_7*ses_{ijk}$

The anova test comparing model 3 to model 4 has a p-value < 2.2e-16, so we reject the null hypothesis and conclude that it makes sense to include student level predictors. Moreover, the Chi-Sq test comparing model 2 to model 4 has a p-value < 2.2e-16, so we conclude that the model with student level predictors (as a block) improves compared to the model with only school-level predictors. 


##Random Slope for Teacher-level predictor varying at school-level

```{r random slope teachers}
#ONE BY ONE
#mathknow
rst.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathknow||schoolid)+(1|classid),data=classroom)
summary(rst.1)
rand(rst.1) #not sig
#yearstea
rst.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+yearstea||schoolid)+(1|classid),data=classroom)
summary(rst.2)
rand(rst.2)
#mathprep
rst.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathprep||schoolid)+(1|classid),data=classroom)
summary(rst.3)
rand(rst.3)
```

\textbf{Question:} Why housepov bad idea? 

\textbf{Answer:} There is only one data point per school, so we cannot have a random slope since we can't even calculate a slope.

##Allowing correlations with random intercepts

```{r correlation with random intercepts}
#ONE BY ONE
#mathknow
rstc.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathknow|schoolid)+(1|classid),data=classroom)
summary(rstc.1)
rand(rstc.1)
#yearstea
rstc.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+yearstea|schoolid)+(1|classid),data=classroom)
summary(rstc.2)
rand(rstc.2) #just over .05
#mathprep
rstc.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+mathprep|schoolid)+(1|classid),data=classroom)
summary(rstc.3)
rand(rstc.3)
```



\textbf{Question:} Anything unusual about the variances? Why might this have occurred? (hint: what did you add to the model?)

\textbf{Answer:} The random slope for mathknow greatly increases in the second model, which is probably due to its 
correlation with the random intercept at the school-level. 

B. note: Also -- weird that correlation among slope and intercept is prefectly -1. This looks like model is not fitting properly -- probably because some classrooms have very few observations so not all parameters are estimated. 



##Random slopes for student-level predictors varying at classroom level

```{r random slope student-level}
#ONE BY ONE
#sex
rss.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex||classid)+(1|schoolid),data=classroom)
summary(rss.1)
rand(rss.1)
#minority
rss.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority||classid)+(1|schoolid),data=classroom)
summary(rss.2)
rand(rss.1)
#SES
rss.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses||classid)+(1|schoolid),data=classroom)
summary(rss.3)
rand(rss.3)
```

\textbf{Question:} why is this a bad idea to include a classroom-level variable with random slopes at classroom-level? 

\textbf{Answer:} It may not explain much variance. 

##Allowing for correlations with random intercepts

```{r correlations}
#ONE BY ONE
#sex
rssc.1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex|classid)+(1|schoolid),data=classroom)
summary(rssc.1)
rand(rssc.1)
#minority
rssc.2 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority|classid)+(1|schoolid),data=classroom)
summary(rssc.2)
rand(rssc.2)
#SES
rssc.3 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses|classid)+(1|schoolid),data=classroom)
summary(rssc.3)
rand(rssc.3)
```

##Random slopes for student-level predictors varying at school level

```{r random slopes student-level}
#ONE BY ONE
#sex
rss.4 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex||schoolid)+(1|classid),data=classroom)
summary(rss.4)
rand(rss.4)
#minority
rss.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority||schoolid)+(1|classid),data=classroom)
summary(rss.5)
rand(rss.5)
#SES
rss.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses||schoolid)+(1|classid),data=classroom)
summary(rss.6) #IS SIG
rand(rss.6)
```

##Allowing for correlations with random intercepts

```{r correlations student-school}
#ONE BY ONE
#sex
rssc.4 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+sex|schoolid)+(1|classid),data=classroom)
summary(rssc.4)
rand(rssc.4)
#minority
rssc.5 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+minority|schoolid)+(1|classid),data=classroom)
summary(rssc.5)
rand(rssc.5) #sig
#SES
rssc.6 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1+ses|schoolid)+(1|classid),data=classroom)
summary(rssc.6)
rand(rssc.6) #not sig




```

\textbf{Question:} Report unusual changes in variance.

\textbf{Answer:} Perhaps most striking is the change in variance for the random slope term on minority. Previously, it was 0. However, it jumps to 343.13 in the correlated model. The variance for the random slope term on SES also increases, but the correlated random slope is not a significant addition to our model according to the rand test results.

##Complex model better?

```{r}
#Take two predictors that had sig random slopes and add to model, test for need of one conditional on the other
#minority is sig for correlated
#ses is sig for uncorrelated
complex <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(0+ses|schoolid)+(1+minority|schoolid)+(1|classid),data=classroom)
summary(complex)
rand(complex)
```

\textbf{Question:} Is the more complex model (with both random slopes in it) justified?

\textbf{Answer:} The complex model is justified since the rand test shows that the random slopes are both statistically significant at the 0.05 level.

The equation for the complex model is given by the following:

$ \hat{Math1st_{ijk}} = \beta_0 + \beta_1*housepov_{k} + \beta_2*mathknow_{jk} + \beta_3*yearstea_{jk} + \beta_4*mathprep_{jk} + \beta_5*sex_{ijk} + \beta_{6k}*ses_{ijk} + \beta_{7k}*minority_{ijk} + \zeta_{0k} + \zeta_{6k} + \zeta_{7k} + eta_{jk} + \epsilon_{ijk}$

where $\zeta_{0k} \sim N(0, \sigma_{\zeta_0}^2),\zeta_{6k} \sim N(0, \sigma_{\zeta_6}^2), \zeta_{7k} \sim N(0, \sigma_{\zeta_7}^2), \eta_{jk} \sim N(0, \sigma_\eta^2),$ and $\epsilon_{ijk} \sim N(0, \sigma_\epsilon^2)$, all independent of each other.

## $V_C$, $V_S$, and $V_E$

\textbf{Question:} For UCM, write down: $V_C$, $V_S$, $V_E$ for the three variance components (simply the estimates). Think of them as possibly varying with a covariate, though.

\textbf{Answer:} For the UCM, $V_C$ = 85.46, $V_S$ = 280.68, and $V_E$ = 1146.80

\textbf{Question:} For the most complicated (all fixed effects) random INTERCEPTS ONLY model, what are: $V_C$, $V_S$, $V_E$?

\textbf{Answer:} For the most complicated fixed effects model with only random intercepts, $V_C$ = 93.89, $V_S$ = 169.45, and $V_E$ = 1064.95.

\textbf{Question:} By what fraction did these each decrease with the new predictors in the model?

\textbf{Answer:} 

\textbf{Question:} Now consider the model with a random slope in ses. What are: $V_C$, $V_S(ses=0)$, $V_E$ ? We need to list 'ses=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in ses at the school level, $V_C$ = 88.56, $V_S(ses=0)$ = 167.98, and $V_E$ = 1035.12.

\textbf{Question:} What are: $V_S(ses=-0.50)$, $V_S(ses=+0.5)$ ?

\textbf{Answer:} In this model, in which the random slope for SES is uncorrelated with the random school-level intercept, $V_S(ses=-0.50) = 167.98 + (-.5)^2*72.50 + 2*(-.5)*0*167.98*72.50 = 186.105$, and $V_S(ses=+0.5) = 167.98 + (.5)^2*72.50 + 2*(.5)*0*167.98*72.50 = 186.105$ 

\textbf{Question:} Now consider the model with a random slope in minority. What are: $V_C$, $V_S(minority=0)$, $V_E$? We need to list 'minority=0' here, or we don't know how to use the slope variance

\textbf{Answer:} For the model with a random slope in minority at the school level, $V_C$ = 86.69, $V_S(minority=0)$ = 381.20, and $V_E$ = 1039.39.

\textbf{Question:} What are: $V_S(minority=0.25)$, $V_S(minority=+0.50)$, $V_S(minority=+0.75)$?

\textbf{Answer:} In this model, in which the random slope for minority is correlated with the random school-level, intercept, 
$V_S(minority=0.25) = 381.20 + (0.25)^2*343.13 + 2*(0.25)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 252.5549$, 

$V_S(minority=+0.50) = 381.20 + (0.50)^2*343.13 + 2*(0.50)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 166.801$, and

$V_S(minority=+0.75) = 381.20 + (0.25)^2*343.13 + 2*(0.25)*(-0.83)*\sqrt{381.20}*\sqrt{343.13} = 123.9384$.

\textbf{Question:} Now consider the model with a random slope in ses & minority. What are: $V_C$, $V_S(minority=0,ses=0)$, $V_E$? We need to list 'ses=0, minority=0' here, or we don't know how to use the slope variance.

\textbf{Answer:} For the model with a random slope in ses & minority, $V_C$ = 80.63, $V_S(minority=0,ses=0)$ = 404.54, and $V_E$ = 1009.73.

\textbf{Question:} What are: $V_S(ses=0,minority=0.50)$, $V_S(ses=0.50,minority=0)$, $V_S(ses= 0.50, minority= 0.50)$?

\textbf{Answer:} In this model, in which the random slope for ses is uncorrelated with the random intercept, but the random slope for minority is correlated with the random intercept,

$V_S(ses=0,minority=0.50) = 404.54 + (0)^2*74.93 + (0.50)^2*336.04 + 2*0*0*404.54*74.93 + 2*(0.50)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 182.5268$,

$V_S(ses=0.50,minority=0) = 404.54 + (0.50)^2*74.93 + (0)^2*336.04 + 2*0.50*0*404.54*74.93 + 2*(0)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 423.2725$

$V_S(ses= 0.50, minority= 0.50) = 404.54 + (0.50)^2*74.93 + (0.50)^2*336.04 + 2*0.50*0*404.54*74.93 + 2*(0.50)*(-0.83)*\sqrt{404.54}*\sqrt{336.04} = 201.2593$

\textbf{Question:} In the last model, what is a "likely" (+/- 1 sd) range for \eta_{0jk}

\textbf{Answer:} For the complex model, the "likely" range for \eta{0jk} is 71.651 to 89.609.

\textbf{Question:} Can we make a similar statement about \zeta_{0k}?

\textbf{Answer:} 

\textbf{Question:} If you had a large value for \eta_{0jk}, would you expect a large or small or "any" value for: the two random slope terms, \zeta_{1k} and \zeta_{2k} for ses and minority?

\textbf{Answer:} 

\textbf{Question:} If you had a large value for \zeta_{0k}, would you expect a large or small or "any" value for: the two random slope terms, \zeta_{1k} and \zeta_{2k} for ses and minority (discuss each separately)?

\textbf{Answer:} 

# Part 2: Clare

##Running initial model

The initial model was run on a smaller dataset with 1081 observations due to missing data. School-level and classroom-level random intercepts are included in the model.

```{r pt2 q1}
#remove missing data -- not ideal, but have to do it for this analysis
classroom <- classroom %>% mutate(Math1st = mathkind + mathgain)
classroom2 <- na.omit(classroom)
#model
new1 <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(1|schoolid)+(1|classid),data=classroom2)
```

##Residual that removes only the "fixed effects"

Below we calculate the residuals that removes only the fixed effects. The boxplot of the residuals shows that there is great variation within schools and that there is a steady linear trend to the residuals, suggesting dependence. 

```{r pt2 q2}
#predicted scores
pred.yhat <- predict(new1,re.form=~0)

#residual
resFE <- classroom2$Math1st-pred.yhat

#show that it's not independent
if (vanillaR) {
ord <- order(unlist(tapply(resFE, classroom2$schoolid, median)))
boxplot(split(resFE, classroom2$schoolid)[ord])
} else {
ggplot(classroom2, aes(x = reorder(schoolid, resFE, FUN = median), y = resFE)) +
geom_boxplot()
}
```

##Residuals for BLUPs random effects

The residuals for the BLUPs random effects are calculated below. The boxplot reveals a similar dependency to the previous plot, though not as pronounced. There doesn't seem to be as high a correlation as there is in the other residuals plot. 

```{r pt2 q3}
#getting predicted zeta_0 and eta_0
ranefs <- ranef(new1)
zeta0 <- ranefs$schoolid[,1]
eta0 <- ranefs$classid[,1]
#indexing
idx.sch <- match(classroom2$schoolid, sort(unique(classroom2$schoolid))) 
idx.cls <- match(classroom2$classid, sort(unique(classroom2$classid))) 
classroom2$zeta0 <- zeta0[idx.sch]
classroom2$eta0 <- eta0[idx.cls]
#now subtract all from outcome
resFE_RE <- classroom2$Math1st-pred.yhat-classroom2$zeta0-classroom2$eta0
#show that it's not independent, but much less correlated than resFE
if (vanillaR) {
ord <- order(unlist(tapply(resFE_RE, classroom2$schoolid, median)))
boxplot(split(resFE_RE, classroom2$schoolid)[ord])
}else{
ggplot(classroom2, aes(x = reorder(schoolid, resFE_RE, FUN = median), y = resFE_RE)) +
geom_boxplot()
}
```

## Examining BLUPs for normality

To examine the BLUPs for mormality, density plots and Q-Q plots were constructed. Both $zeta_0$ and $eta_0$ appear to be normal, with a few poossible outliers near the tails. 

```{r blups normality, fig.width=6, fig.height=4}
par(mfrow=c(2,2))
plot(density(zeta0), main ="Normality Check for Zeta")
plot(density(eta0), main = "Normality Check for Eta")
#looking good
qqnorm(zeta0);qqline(zeta0)
qqnorm(eta0);qqline(eta0)
#looking good
```

##Simulation
*if In a simpler setting, with no classroom level, if H0: \sigma^2_{\zeta_0}=0 were true, and you sampled 100 schools of size 10, what would be a potential expected estimate of \sigma_{\zeta_0} if you know that \sigma_{\epsilon}=1 ?  HINT: the Central Limit Theorem applies.  Simulate to find out.
```{r blups simulation}
set.seed(10314)
school.sim <- matrix(1,10,100)
for (i in 1:100){
school.sim[,i] <- rnorm(10,mean=0, sd=1)
}
plot(density(school.sim), main = "Density of Zeta0")
paste("A potential estimate of sigma_{zeta_0}",mean(school.sim))
```

##New Complex Model

We now include an uncorrelated random slope at the school-level for minority. 

```{r new complex model}
classroom <- read.csv("classroom.csv")
classroom <- classroom %>% mutate(Math1st = mathkind+mathgain)
classroom2 <- na.omit(classroom)
newcomplex <-lmer(Math1st~housepov+mathknow+yearstea+mathprep+sex+minority+ses+(minority|schoolid)+(1|classid),data=classroom2)
summary(newcomplex)
```

##Manually calculate residuals for fixed effects

In the new model, we see a similar pattern of dependency. There is a general positive, linear trend to the residuals, and there is heterogeneity of variance across and within schools. These findings all suggest dependence.

```{r complex resFE}
#predicted scores
pred.yhat2 <- predict(newcomplex,re.form=~0)

#residual
resFE2 <- classroom2$Math1st-pred.yhat2

#show that it's not independent
if (vanillaR) {
ord <- order(unlist(tapply(resFE2, classroom2$schoolid, median)))
boxplot(split(resFE2, classroom2$schoolid)[ord])
} else {
ggplot(classroom2, aes(x = reorder(schoolid, resFE2, FUN = median), y = resFE2)) +
geom_boxplot()
}
```

##Residuals from BLUPs random effects

The residuals from the BLUPs random effects are calculated below. The boxplot of the residuals appears to be only slightly correlated, partly due to the uptake near the final set of schools on the x-axis. Although the correlation of the residuals is probably near 0, there is still enough variation within schools, and enough of a correlation in the data to suggest dependence. 

```{r complex blup residuals}
#getting predicted zeta_0 and eta_0
ranefs2 <- ranef(newcomplex)
zeta0c <- ranefs$schoolid[,1]
eta0c <- ranefs$classid[,1]
zeta1c <- ranefs2$schoolid[,2]
#indexing
idx.sch <- match(classroom2$schoolid, sort(unique(classroom2$schoolid))) 
idx.cls <- match(classroom2$classid, sort(unique(classroom2$classid))) 
classroom2$zeta0c <- zeta0c[idx.sch]
classroom2$eta0c <- eta0c[idx.cls]
classroom2$zeta1c <- zeta1c[idx.sch]
#now subtract all from outcome
resFE_RE2 <- classroom2$Math1st-pred.yhat-classroom2$zeta0c-classroom2$eta0c-(classroom2$minority*classroom2$zeta1c)
#show that it's not independent, but much less correlated than resFE
if (vanillaR) {
ord <- order(unlist(tapply(resFE_RE2, classroom2$schoolid, median)))
boxplot(split(resFE_RE2, classroom2$schoolid)[ord])
}else{
ggplot(classroom2, aes(x = reorder(schoolid, resFE_RE2, FUN = median), y = resFE_RE2)) +
geom_boxplot()
}
```

##Examining Normality of BLUPs

Below we examine the normality of $\zeta_0$ and $eta_0$. The density and Q-Q plots for both $zeta_0$ and $eta_0$ suggest normality, with a possibility of a few outliers near the tails. 

```{r complex blup normality}
par(mfrow=c(2,2))
plot(density(zeta0c), main ="Normality Check for Zeta")
plot(density(eta0c), main = "Normality Check for Eta")
# eta looks pretty normal
#zeta not so much
qqnorm(zeta0c, main = "Q-Q Plot for Zeta");qqline(zeta0c)
qqnorm(eta0c, main = "Q-Q Plot for Eta");qqline(eta0c)
#zeta looking good, but with a few possible outliers
#eta good too, with few outliers. 
```

## Plotting $\zeta_0$ versus $\zeta_1$

The correlation between $\zeta_0$ and $zeta_1$ in the output is -0.83. The graph below suggests a moderate negative trend, but there are some outliers that do not support this trend. Rather, they seem to be positively related. 

*Note: the labels were put in rainbow in order to better discern their locations.

```{r plotting the zetas}
plot(classroom2$zeta0c,classroom2$zeta1c, main = "Zeta0 vs. Zeta1", ylab = "Zeta1",xlab = "Zeta0", pch=19)
 text(classroom2$zeta0c,classroom2$zeta1c, labels = classroom2$schoolid, cex = 0.8, col = rainbow(100))
```


##Tracking down outliers

The outliers from the plots above can be tracked down by examining the data points via their IDs. 

```{r complex outliers}
classroom2$zeta0c[classroom2$schoolid==45][[1]]/classroom2$zeta1c[classroom2$schoolid==45][[1]]
classroom2$zeta0c[classroom2$schoolid==68][[1]]/classroom2$zeta1c[classroom2$schoolid==68][[1]]
classroom2$zeta0c[classroom2$schoolid==30][[1]]/classroom2$zeta1c[classroom2$schoolid==30][[1]]

#there seems to be a trend here that the zeta0/zeta1 ratio is > 3, so let's filter it out
outliers <- classroom2 %>% filter(zeta0c/zeta1c > 3) %>% select(zeta0c,zeta1c,schoolid,minority)
#now let's make sure the IDs from the plot are showing up here
unique(outliers$schoolid)
#They are! Now what's going on with minority?
table(outliers$minority)
```

It seems like the positive trend in the data is being driven by schools in which all the students are primarily minorities. 

# Part 3: Bianca

## Create person-period file

```{r person-period file}
#re-read data
classroom <- read.csv("classroom.csv")
classroom2 <- na.omit(classroom)
#new variables
classroom2 <- classroom2 %>% mutate(math0 = mathkind) %>% mutate(math1 = mathkind+mathgain)
#reshape the data
class_pp <- reshape(classroom2, varying = c("math0", "math1"), v.names = "math", timevar = "year",
times = c(0, 1), direction = "long")
```

## Initial longitudinal model

The equation for the model below:

$$Math_{tijk} = b_0 + \zeta_{0k} + b_1*Time_{tijk} + \epsilon_{tijk}$$
where $\zeta_{0k} \sim N(0, \sigma_{zeta0}^2)$ and $\epsilon_{tijk} \sim N(0, \sigma_{\epsilon}^2)$

```{r pp model1}
fit1 <- lmer(math ~ year + (1|schoolid), data = class_pp)
summary(fit1)
```

## Add child-level random intercept

$Math_{tijk} = b_0 + \delta_{0tijk} + \zeta_{0k} + b_1*Time_{tijk} + \epsilon_{tijk}$
where $\delta_{0tijk} \sim N(0, \sigma_{\delta_0}^2), \zeta_{0k} \sim N(0, \sigma_{zeta0}^2)$ and $\epsilon_{tijk} \sim N(0, \sigma_{\epsilon}^2)$ independetly of one another. 


```{r pp model2}
fit2 <- lmer(math ~ year + (1|schoolid/childid), data = class_pp)
summary(fit2)
```

In model 1 the variance $\sigma_{\zeta_0}^2 = 377$ and in model 2  $\sigma_{\zeta_0}^2 = 293.2$. 
In model 1 , the varaince for $\sigma_{\epsilon}^2 = 1288$ and in model 2 $\sigma_{\epsilon_0}^2 = 602.2$. We note that including child-level variation leads to a decrease in the variance of both the random effects. 

## Compute Pseudo-R^2

Compute a pseudo R^2 relating the between school variation and ignoring between students in the same school. 

We calculate this as :

$$\frac{\sigma_{\zeta_0}^2(M_0) - \sigma_{\zeta_0}^2(M_1)}{\sigma_{\zeta_0}^2(M_0)} = \frac{377-293.2}{377} = 0.22$$
The between-school variance is reduced by 22% with the introduction of student random effect?

\textbf{Does the total variation stay about the same?}

```{r pp mods totvariance}
tot_m0 = 337 + 1288
tot_m1 = 722 + 293.2 + 602.2 
paste("Tot variance for model 0 : ", tot_m0)
paste("Tot variance for model 1: ", tot_m1)

```

There is only a slightly decrease in the total variance between Model 0 and Model1. 

## Add a random slope for time trend

```{r pp mod3}
fit3 = lmer(math ~ year + (1 + year|| schoolid) + (1|childid), data = class_pp)
summary(fit3)
```


## Generate the BLUPs for this model (fit 3)
Examine then whether the independence between zeta0 and zeta1 is reflected in a scatterplot of these two sets of effects. 

***CHECK: is this what he is asking? i.e. check correlation between zeta0 and zeta1

```{r pp blups}
pp_ranefs <- ranef(fit3)




if (vanillaR) {
plot(pp_ranefs$schoolid[,2],pp_ranefs$schoolid[,1]) 
}else{
ggplot(pp_ranefs$schoolid, aes(x = pp_ranefs$schoolid[,2], y = pp_ranefs$schoolid[,1] )) +
geom_point() + labs(x = "Zeta_{0k} BLOPs", y = "Zeta_{1k} BLOps") + geom_hline( yintercept = 0)
}



#cor(pp_ranefs$schoolid[,2], pp_ranefs$schoolid[,1])
```

Looks a bit like there is **some** negative correlation but not much?

## Heteroscedasticity in the random effects

\textbf{Question:} What are: $V_S(year = 0)$, $V_S(year = 1)$?

```{r}
summary(fit3)
```
The model we are considering is           **(check subscripts pls)

$Math_{tijk} = b_0 + \delta_{0tijk} + \zeta_{0k} + (b_1 + \zeta_{1k})Time_{tijk} + \epsilon_{tijk}$

So we have that (in this model where we are forcing correlation of 0 between slope and intercept):

-  $V_S(year = 0) = \sigma_{\zeta_{0k}}^2 = 85.96$
-  $V_S(year = 1) = \sigma_{\zeta_{0k}}^2 + \sigma_{\zeta_{1k}}^2 = 85.96 + 315.69 = 401.65$

## Run model by year

```{r}
class_year0 = class_pp[class_pp$year == 0,]



fit4 = lmer(math ~ (1 | schoolid), data = class_year0)
summary(fit4)


class_year1 = class_pp[class_pp$year == 1,]
fit5 = lmer(math ~ (1 | schoolid), data = class_year1)
summary(fit5)



```

No, we do no ge the same estimates for the variances by running the model separately. 

## Allow for correlation 

```{r}
fit6 = lmer(math ~ year + (1 + year| schoolid) + (1|childid), data = class_pp)
summary(fit6)

```

Correlation beteween $\zeta_0$ and $\zeta_1$ = -0.53. 
Statistically significant?

So we have that (in this model where we are forcing correlation of 0 between slope and intercept):

-  $V_S(year = 0) = \sigma_{\zeta_{0k}}^2 = 373.5$
-  $V_S(year = 1) = \sigma_{\zeta_{0k}}^2 + \sigma_{\zeta_{1k}}^2 + 2\rho_{01}\sigma_{\zeta_{0k}}\sigma_{\zeta_{1k}}= 373.5 + 112.4 - 2*0.53*\sqrt{373.5}\sqrt{112.4} = 268.7$

These are a lot closer to the school variances that result from fitting the models for the two years separately ( in which we have $\sigma_{zeta}^2$ respectively be 364 for year 0 and 279 for year 1. 
